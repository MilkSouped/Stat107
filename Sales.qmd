---
title: "Sales"
author: "Ethan Chan, Alex Back, Darren Kuboyama"
date: "12/11/2024"
format:
  html:
    toc: true
    toc-location: left
    embed-resources: true
    code-line-numbers: true
---

**Scientific Question: How do customer engagement metrics, sentiments, and feedback patterns relate to product performance and differentiate top-rated from bottom-rated products across categories?**

We begin by adding the tidyverse library for later uses.

#Packages

```{r}
library(tidyverse)
library(tidytext) 
```

Next we read in the product information into a dataframe and then combine all the review csvs into one single dataframe to later data clean.

```{r}
product_info <- read.csv('product_info.csv')
review_files <- list.files(pattern = "reviews_.*\\.csv") #merge csvs
reviews <- do.call(rbind, lapply(review_files, read.csv)) #make it into a dataframe
```

Now we narrow it down to the metrics we want to look at, specifically the categories, reviews, ratings, and the love counts.

```{r}
product_data <- product_info %>%
  select(product_id, product_name, brand_id, primary_category, secondary_category, tertiary_category, rating, reviews, loves_count) %>%
  filter(!(secondary_category == "" & tertiary_category == ""))
```

Now we sort them into the top 3 and worst 3 based on the secondary category.

```{r}
top_3 <- product_data %>%
  group_by(secondary_category) %>%
  arrange(desc(rating), desc(reviews), desc(loves_count)) %>% 
  slice_head(n = 3) %>%  
  ungroup()

worst_3 <- product_data %>%
  group_by(secondary_category) %>%
  arrange(rating, reviews, loves_count) %>%  
  slice_head(n = 3) %>%  
  ungroup()

combined <- bind_rows(
  top_3 %>% mutate(category = "Top 3"),
  worst_3 %>% mutate(category = "Worst 3")
)

```

Now we might look at some trends, lets look at the average rating, reviews, and loves of the top 3 and worst 3.

```{r}
# Combined summary for Top 3 and Worst 3
summary_overall <- combined %>%
  group_by(category) %>%
  summarize(
    avg_rating = mean(rating, na.rm = TRUE),
    avg_reviews = mean(reviews, na.rm = TRUE),
    avg_loves = mean(loves_count, na.rm = TRUE)
  )
summary_category <- combined %>%
  group_by(secondary_category, category) %>%
  summarize(
    avg_rating = mean(rating, na.rm = TRUE),
    avg_reviews = mean(reviews, na.rm = TRUE),
    avg_loves = mean(loves_count, na.rm = TRUE),
    .groups = "drop"
  )
```

We can now start comparing the rating, reviews, and love counts for the top 3 and bottom 3 products of each category by looking at some graphs.

#Graphs

```{r}
top_bottom_moisturizers <- combined %>%
  filter(secondary_category == "Moisturizers") %>%
  filter(category %in% c("Top 3", "Worst 3"))  # Grab Top 3 and Worst 3

#print(top_bottom_moisturizers) Testing that the correct products are printed

ggplot(top_bottom_moisturizers, aes(x = reorder(product_name, rating), y = rating, fill = category)) +
  geom_bar(stat = "identity", width = 0.5) +
  labs(
    title = "Top 3 and Bottom 3 Ratings for Moisturizers",
    x = "Product Name",
    y = "Rating"
  ) +
  coord_flip() +  # flip the axis for better visibility
  theme_minimal() +
  theme(
    axis.text.y = element_text(size = 6),  
    axis.text.x = element_text(size = 6)
  )
```

An important detail we can see just from visualizing the product names in this graph is that popular ingredients like Hyaluronic Acid does not guarantee a high rating. In fact, it is in the Worst 3 category. In contrast ingredients like Niacinamide or products that target dark spots and the skin's texture seem to be dominating the Top 3.

Now lets narrow this down to gain a deeper understanding of the metric's correlations. This will give us a deeper understanding of whether or not the high ratings are linked to customer engagement and the relationship of these metrics could be a strength or weakness in a company's product.

```{r}
# Scatterplot: Rating vs Reviews
ggplot(combined, aes(x = reviews, y = rating)) +
  geom_point(alpha = 0.6, color = "blue") +
  geom_smooth(method = "lm", se = FALSE, color = "red") +
  labs(
    title = "Relationship Between Reviews and Rating",
    x = "Number of Reviews",
    y = "Rating"
  ) +
  theme_minimal()

# Scatterplot: Rating vs Loves Count
ggplot(combined, aes(x = loves_count, y = rating)) +
  geom_point(alpha = 0.6, color = "green") +
  geom_smooth(method = "lm", se = FALSE, color = "red") +
  labs(
    title = "Relationship Between Loves Count and Rating",
    x = "Loves Count",
    y = "Rating"
  ) +
  theme_minimal()

# Scatterplot: Loves Count vs Reviews
ggplot(combined, aes(x = reviews, y = loves_count)) +
  geom_point(alpha = 0.6, color = "purple") +
  geom_smooth(method = "lm", se = FALSE, color = "red") +
  labs(
    title = "Relationship Between Reviews and Loves Count",
    x = "Number of Reviews",
    y = "Loves Count"
  ) +
  theme_minimal()


```

The first graph, showing the relationship between reviews and ratings, also exhibits a weak positive correlation. Products with more reviews tend to have slightly higher ratings, but most ratings remain in the 4 to 5 range, regardless of review count. Reviews highlight engagement and visibility, while ratings reflect customer satisfaction. The clustering of ratings indicates that while customer satisfaction is generally high, reviews are a more dynamic metric for measuring product visibility and interaction.

The second graph, focusing on the relationship between loves count and ratings, reveals a weak positive correlation. While products with higher loves tend to have slightly higher ratings, the relationship is not strong, and ratings cluster around 4 to 5 regardless of popularity. Loves count measures customer loyalty and emotional appeal, while ratings assess satisfaction. The lack of a strong correlation suggests that a product's popularity does not always translate directly into higher satisfaction. This emphasizes the importance of analyzing these metrics together to capture a productâ€™s full performance.

The third graph, examining the relationship between reviews and loves count, shows a strong positive correlation, indicating that products with higher reviews also tend to have higher loves. This suggests that customer interaction through reviews aligns closely with emotional resonance and loyalty, as measured by loves count. Loves count reflects a product's desirability and brand appeal, while reviews showcase customer engagement and feedback. Products that perform well in both metrics demonstrate strong marketing and customer alignment.

Quantitatively, the correlation between reviews and loves is significantly stronger compared to the relationships between reviews and ratings or ratings and loves. This suggests that products with higher reviews are not only engaging customers but are also more likely to be loved, making the combination of these metrics a holistic indicator of product performance.

```{r}
correlations <- combined %>%
  group_by(secondary_category) %>%
  summarize(
    corr_reviews_rating = cor(reviews, rating, use = "complete.obs"),  
    corr_rating_loves = cor(rating, loves_count, use = "complete.obs"),
    corr_reviews_loves = cor(reviews, loves_count, use = "complete.obs"), 
    .groups = "drop"
  )

correlations


```

This table reveals something interesting. The worst 3 products tend to have a higher average review and love count compared to the top 3 products which have fewer reviews and love counts. This suggests that high visibility and customer engagement (throguh reviews) dont always translate to positive sentiments or high ratings. instead these metrics could indicate polarizing products that attract significant attention but also receive miexed or negative feedback. However, the top 3 products,despite having fewer reviews and loves maintain a higher average rating which could mean they deliver consistent qualtiy and satisifcation to a samller but liekly more loyal audience. This observation could enforce the idea taht quantity of engagement throguh reviews or loves does not necessairly equate to a quality of sentiment through ratings. Understanding this dynamic could help a brand focus on improving their underperformign product with high visibility to turn into loyalty and satsifcation. But why might a low rated product have such low love counts you might ask? This could be due to either strong marketing, or a brand's reputation, a subset of users who strongly dislike and those who like, historical popularity that has declined, brand loyalty, or just simply an intent to purchase but never did.

```{r}
summary_metrics <- combined %>%
  group_by(category) %>%
  summarize(
    avg_rating = mean(rating, na.rm = TRUE),
    avg_reviews = mean(reviews, na.rm = TRUE),
    avg_loves = mean(loves_count, na.rm = TRUE),
    .groups = "drop"
  )

print(summary_metrics)

ggplot(summary_metrics, aes(x = category, y = avg_rating, fill = category)) +
  geom_bar(stat = "identity") +
  labs(
    title = "Average Rating Across Categories (Top 3 vs Worst 3)",
    x = "Category",
    y = "Average Rating"
  ) +
  theme_minimal() 

ggplot(summary_metrics, aes(x = category, y = avg_reviews, fill = category)) +
  geom_bar(stat = "identity") +
  labs(
    title = "Average Reviews Across Categories (Top 3 vs Worst 3)",
    x = "Category",
    y = "Average Reviews"
  ) +
  theme_minimal() 

ggplot(summary_metrics, aes(x = category, y = avg_loves, fill = category)) +
  geom_bar(stat = "identity") +
  labs(
    title = "Average Loves Across Categories (Top 3 vs Worst 3)",
    x = "Category",
    y = "Average Loves"
  ) +
  theme_minimal() 

```

```{r}
broad_category_summary <- combined %>%
  filter(primary_category != "Mini Size") %>% 
  mutate(
    broad_category = ifelse(is.na(primary_category) | primary_category == "", "Other", primary_category)  
  ) %>%
  group_by(broad_category, category) %>%  
  summarize(
    avg_rating = mean(rating, na.rm = TRUE),   
    avg_reviews = mean(reviews, na.rm = TRUE), 
    avg_loves = mean(loves_count, na.rm = TRUE), 
    .groups = "drop"
  )

#avg rating
ggplot(broad_category_summary, aes(x = reorder(broad_category, avg_rating), y = avg_rating, fill = category)) +
  geom_bar(stat = "identity", position = "dodge") +
  coord_flip() +
  labs(
    title = "Average Ratings by Primary Categories (Top 3 vs Worst 3)",
    x = "Primary Category",
    y = "Average Rating",
    fill = "Category"
  ) +
  theme_minimal()

#avg review
ggplot(broad_category_summary, aes(x = reorder(broad_category, avg_reviews), y = avg_reviews, fill = category)) +
  geom_bar(stat = "identity", position = "dodge") +
  coord_flip() +
  labs(
    title = "Average Reviews by Primary Categories (Top 3 vs Worst 3)",
    x = "Primary Category",
    y = "Average Reviews",
    fill = "Category"
  ) +
  theme_minimal()

#avg love count
ggplot(broad_category_summary, aes(x = reorder(broad_category, avg_loves), y = avg_loves, fill = category)) +
  geom_bar(stat = "identity", position = "dodge") +
  coord_flip() +
  labs(
    title = "Average Loves Count by Primary Categories (Top 3 vs Worst 3)",
    x = "Primary Category",
    y = "Average Loves Count",
    fill = "Category"
  ) +
  theme_minimal()

```

To better understand the performance of products across primary categories, we analyzed the data based on average ratings, reviews, and loves counts for the top 3 and bottom 3 products in each category. Mini-sized products were excluded from this analysis to ensure the results focused on the full-sized offerings, as mini sizes might skew the findings without offering unique insights. This breakdown allows for a clearer understanding of customer engagement, satisfaction, and loyalty within each category.

The results reveal several key trends. In skincare, the worst 3 products surprisingly received higher reviews and loves counts compared to the top 3 products. This suggests that skincare products in the worst-performing group still garner significant customer interest but likely fail to meet expectations in terms of quality or effectiveness. This pattern could point to either high demand for these products, driven by strong marketing or desirable features, or simply a broader audience that ended up being dissatisfied with the results. This highlights the importance of investigating variability within this category, as high engagement does not necessarily translate into high satisfaction.

Conversely, Tools & Brushes emerge as a standout category, excelling across all metrics for the top 3 products. This indicates consistent quality and customer satisfaction, where tools and brushes meet or exceed expectations. The strong performance in reviews and loves counts reflects a loyal customer base and suggests that functionality and quality play critical roles in shaping consumer opinions in this category. Tools & Brushes demonstrate that delivering reliable, high-quality products can lead to success across all performance metrics.

Other categories, such as Makeup, Hair, and Bath & Body, reveal more mixed patterns, where engagement levels and satisfaction are not always aligned. For example, some products in these categories have high reviews or loves counts but still fall into the bottom 3 due to lower ratings. This suggests that polarizing preferences or niche appeals might influence customer perceptions, with some consumers finding these products highly satisfactory while others are left disappointed. Fragrance, in particular, shows lower overall reviews and loves counts for both top and bottom performers, reflecting its position as a more specialized category that caters to a specific audience.

These findings emphasize the complex dynamics between engagement, loyalty, and satisfaction. While reviews and loves counts provide valuable insights into customer interest and emotional connection to products, they do not always correlate directly with ratings, which reflect overall satisfaction and quality. Skincare, for instance, underscores the need for brands to explore why high customer interest does not always lead to high satisfaction, while the consistent performance of Tools & Brushes highlights the importance of quality in driving positive outcomes.

To gain a more nuanced understanding of these trends, it is essential to dive deeper into customer review sentiments. This would help uncover the specific reasons behind high engagement but low satisfaction in certain categories and identify potential areas for improvement. Additionally, leveraging the success of consistently high-performing categories like Tools & Brushes could provide valuable insights into how other categories might enhance their appeal and performance. Understanding these patterns will allow brands to refine their strategies, improve under performing products, and build on the strengths of their most successful offerings.

Now lets move onto reviews

```{r}
reviews_filtered <- reviews %>%
  filter(product_id %in% combined$product_id) %>%
  select(product_id, review_text, is_recommended, helpfulness, 
        total_feedback_count, total_neg_feedback_count, total_pos_feedback_count)

reviews_cleaned <- reviews_filtered %>%
  filter(!is.na(review_text) & review_text != "") %>%
  unnest_tokens(word, review_text) %>%
  filter(str_detect(word, "[a-zA-Z']")) %>%
  mutate(word = tolower(word)) %>%
  anti_join(stop_words, by = "word")

review_sentiments <- reviews_cleaned %>%
  inner_join(get_sentiments("bing"), by = "word") %>%
  count(product_id, sentiment) %>%
  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %>%
  mutate(
    sentiment_score = positive - negative,
    sentiment_ratio = positive / (positive + negative + 1)
  )
```

```{r}
# Join sentiment scores with filtered reviews
reviews_with_sentiments <- reviews_filtered %>%
  left_join(review_sentiments, by = "product_id")  # Add sentiment data

# Summarize sentiment with additional metrics
review_summary <- reviews_with_sentiments %>%
  group_by(product_id) %>%
  summarize(
    avg_sentiment = mean(sentiment_score, na.rm = TRUE),  # Use sentiment_score directly
    avg_helpfulness = mean(helpfulness, na.rm = TRUE),
    avg_total_feedback = mean(total_feedback_count, na.rm = TRUE),
    avg_positive_feedback = mean(total_pos_feedback_count, na.rm = TRUE),
    avg_negative_feedback = mean(total_neg_feedback_count, na.rm = TRUE),
    avg_is_recommended = mean(as.numeric(is_recommended), na.rm = TRUE),  # Convert logical to numeric
    .groups = "drop"
  )

```

```{r}
combined_sentiment <- combined %>%
    left_join(review_summary, by = "product_id") %>%
    drop_na(avg_sentiment, rating) %>% 
    filter(is.finite(avg_sentiment), is.finite(rating))

combined_sentiment <- combined_sentiment %>%
    filter(!is.na(avg_helpfulness), !is.na(avg_sentiment)) %>%
    filter(is.finite(avg_helpfulness), is.finite(avg_sentiment))

summary_by_category <- combined_sentiment %>%
  group_by(category) %>%
  summarize(
    avg_rating = mean(rating, na.rm = TRUE),
    avg_sentiment = mean(avg_sentiment, na.rm = TRUE),
    avg_helpfulness = mean(avg_helpfulness, na.rm = TRUE),
    avg_total_feedback = mean(avg_total_feedback, na.rm = TRUE),
    avg_positive_feedback = mean(avg_positive_feedback, na.rm = TRUE),
    avg_negative_feedback = mean(avg_negative_feedback, na.rm = TRUE),
    avg_is_recommended = mean(avg_is_recommended, na.rm = TRUE),
    .groups = "drop"
  )

# View summary
print(summary_by_category)


```

```{r}
ggplot(combined_sentiment, aes(x = avg_sentiment, y = rating, color = category)) +
  geom_point(alpha = 0.6) +
  geom_smooth(method = "lm", se = FALSE) +
  labs(
    title = "Sentiment Score vs. Rating",
    x = "Average Sentiment Score",
    y = "Rating"
  ) +
  theme_minimal()
  
```
This graph illustrates the relationship between sentiment score and product 
rating, separating the top 3 and worst 3 products. The top 3 products show 
consistently high ratings (close to 5) and positive sentiment scores, indicating
strong alignment between customer satisfaction and emotional responses. In 
contrast, the worst 3 products display more variability, with lower ratings and 
wider ranges of sentiment scores. While positive sentiment generally correlates 
with higher ratings, the variability in sentiment for poorly rated products 
suggests dissatisfaction or unmet expectations. This comparison highlights how 
sentiment analysis complements ratings in identifying customer perceptions of 
product effectiveness.

```{r}
ggplot(combined_sentiment, aes(x = avg_helpfulness, y = avg_sentiment, color = category)) +
  geom_point(alpha = 0.6) +
  geom_smooth(method = "lm", se = FALSE) +
  labs(
    title = "Helpfulness vs. Sentiment Score",
    x = "Average Helpfulness Rating",
    y = "Average Sentiment Score"
  ) +
  theme_minimal()

```
This graph explores the alignment between review helpfulness and sentiment 
scores for top and worst-performing products. For the top 3 products, reviews 
deemed helpful often exhibit higher sentiment scores, reflecting a direct 
connection between perceived review quality and positivity. However, for the 
worst 3 products, helpfulness ratings show little alignment with sentiment, as 
reviews for these products exhibit generally low sentiment regardless of their 
perceived helpfulness. This suggests that helpful reviews for top-performing 
products reinforce their appeal, while reviews for poorly performing products 
may highlight issues without offering actionable positivity.

```{r}
combined_sentiment <- combined_sentiment %>%
  mutate(feedback_ratio = avg_positive_feedback / avg_total_feedback)

ggplot(combined_sentiment, aes(x = feedback_ratio, y = avg_sentiment, color = category)) +
  geom_point(alpha = 0.6) +
  geom_smooth(method = "lm", se = FALSE) +
  labs(
    title = "Positive Feedback Ratio vs. Sentiment Score",
    x = "Positive Feedback Ratio",
    y = "Average Sentiment Score"
  ) +
  theme_minimal()

```
This graph examines how the ratio of positive feedback correlates with sentiment scores across top and worst-performing products. Top 3 products show a clear relationship between high positive feedback ratios and strong sentiment scores, confirming customer satisfaction and engagement. On the other hand, the worst 3 products display more variability in sentiment scores, even when positive feedback ratios are relatively high. This indicates that low-performing products may still receive some positive feedback, but the overall sentiment reflects dissatisfaction or mixed experiences. The distinction reinforces the importance of both feedback quality and sentiment in evaluating product success.

```{r}
feedback_data <- combined_sentiment %>%
  gather(key = "feedback_type", value = "feedback_count", avg_positive_feedback, avg_negative_feedback)

ggplot(feedback_data, aes(x = category, y = feedback_count, fill = feedback_type)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(
    title = "Comparison of Positive and Negative Feedback by Category",
    x = "Category",
    y = "Feedback Count",
    fill = "Feedback Type"
  ) +
  theme_minimal()


```

This bar chart compares the average positive and negative feedback counts for 
the top 3 and worst 3 products. Top-performing products receive overwhelmingly 
positive feedback, with minimal negative responses, underscoring their 
effectiveness and popularity. Conversely, the worst-performing products exhibit 
a more balanced feedback profile, with lower overall feedback counts and a 
higher proportion of negative responses. This contrast highlights the role of 
feedback in driving product reputation, with positive feedback reinforcing 
customer loyalty for top products, while mixed feedback signals issues for the 
worst performers.

```{r}
combined_sentiment <- combined_sentiment %>%
  mutate(feedback_ratio = avg_positive_feedback / avg_total_feedback)

model <- lm(rating ~ avg_sentiment + avg_helpfulness + feedback_ratio + category, data = combined_sentiment)

summary(model)

```
To quantify these relationships, we performed a linear regression analysis. The 
model predicts product ratings based on average sentiment, helpfulness, feedback
ratio, and category. The results showed that while sentiment scores and feedback
ratios alone werenâ€™t statistically significant predictors, the distinction 
between top 3 and worst 3 categories was highly significant. Products in the 
worst 3 categories, on average, had ratings about 2.5 points lower than those in
the top 3. This highlights that category plays a critical role in 
differentiating product performance.

The model itself had a high R-squared value of 0.89, meaning it explains about 
89% of the variance in product ratings, showing that our chosen predictors 
together provide a strong explanation of product performance.

So, what does this mean? By combining metrics like ratings, reviews, and 
sentiment, we gain a comprehensive view of what drives product performance. This
analysis helps us identify that most top products earned its score because of 
its quality / effectiveness being the main driver of it being well-loved while 
reviews and customer sentiments did not play a huge factor in the product's 
ratings and love count. 




